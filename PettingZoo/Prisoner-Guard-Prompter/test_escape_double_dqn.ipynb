{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "input_shape = [4]   # env.observation_space.shape\n",
    "n_outputs = 5       # env.action_space.n\n",
    "\n",
    "initializer = tf.keras.initializers.RandomUniform(minval=-0.01, maxval=0.01)\n",
    "\n",
    "online_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=input_shape),\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\", kernel_initializer=initializer,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(32, kernel_initializer=initializer,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])\n",
    "# target model is just clone of online model\n",
    "target_model = tf.keras.models.clone_model(online_model)  # clone the model's architecture\n",
    "target_model.set_weights(online_model.get_weights())  # copy the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(model, state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
    "        return np.argmax(Q_values)\n",
    "\n",
    "def play_one_step(env, model, obs, epsilon, replay_buffer):\n",
    "    action = epsilon_greedy_policy(model, obs, epsilon)\n",
    "    actions = {\"prisoner\": action}\n",
    "    next_obs, reward, term, trunc, info = env.step(actions)\n",
    "    next_obs = next_obs[\"prisoner\"] / 10\n",
    "    reward = reward[\"prisoner\"]\n",
    "    term = term[\"prisoner\"]\n",
    "    trunc = trunc[\"prisoner\"]\n",
    "\n",
    "    replay_buffer.append((obs, action, reward, next_obs, term, trunc))\n",
    "    \n",
    "def sample_experiences(replay_buffer, batch_size):\n",
    "    # print(f\"{replay_buffer = }\")\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices] # samptling\n",
    "    n_fields = len(batch[0])\n",
    "    sample = []\n",
    "    # \"transpose\":\n",
    "    for field_index in range(n_fields): # obs, actions, rewards, next_obs, dones, truncateds\n",
    "        field_data = []\n",
    "        for experience in batch:\n",
    "            field_data.append(experience[field_index])\n",
    "        sample.append(np.array(field_data))\n",
    "    return sample  # [obs, actions, rewards, next_obs, dones, truncateds]\n",
    "\n",
    "def training_step(online_model, target_model, buffer, optimizer, loss_fn, batch_size, discount):\n",
    "    experiences = sample_experiences(buffer, batch_size)\n",
    "    obs, actions, rewards, next_obs, terms, truncs = experiences\n",
    "\n",
    "    next_Q_values = online_model.predict(next_obs, verbose=0)  # â‰  target_model.predict()\n",
    "    best_next_actions = next_Q_values.argmax(axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    max_next_Q_values = (target_model.predict(next_obs, verbose = 0) * next_mask\n",
    "                        ).sum(axis=1)\n",
    "\n",
    "    runs = 1.0 - (terms | truncs)\n",
    "    target_Q_values = rewards + runs * discount * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    \n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = online_model(obs)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        \n",
    "        # print(f\"{target_Q_values = }, {Q_values = }\")\n",
    "\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "        print(f\"loss: {float(loss)}\")\n",
    "    grads = tape.gradient(loss, online_model.trainable_variables)\n",
    "    if float(loss) < 5:\n",
    "        optimizer.apply_gradients(zip(grads, online_model.trainable_variables))\n",
    "    else:\n",
    "        print(f\"reject, loss = {float(loss)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import escape\n",
    "\n",
    "max_cycles = 152\n",
    "env = escape.env(render_mode=None, max_cycles=max_cycles)\n",
    "# env.reset(seed=42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 16\n",
    "discount_factor = 0.95\n",
    "lr = 0.003\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=lr)\n",
    "loss_fn = tf.keras.losses.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 52, Steps: 154, eps: 0.898 rewards = -90.427 loss: 4.459138870239258\n",
      "Episode: 53, Steps: 154, eps: 0.896 rewards = -395.276 loss: 5.016085624694824\n",
      "reject, loss = 5.016085624694824\n",
      "Episode: 54, Steps: 154, eps: 0.894 rewards = -383.685 loss: 5.926015853881836\n",
      "reject, loss = 5.926015853881836\n",
      "Episode: 55, Steps: 154, eps: 0.892 rewards = 172.280 loss: 5.7874298095703125\n",
      "reject, loss = 5.7874298095703125\n",
      "Episode: 56, Steps: 154, eps: 0.890 rewards = -87.611 loss: 4.473961353302002\n",
      "Episode: 57, Steps: 154, eps: 0.888 rewards = -227.665 loss: 4.321186542510986\n",
      "Episode: 58, Steps: 154, eps: 0.886 rewards = -87.065 loss: 3.970384120941162\n",
      "Episode: 59, Steps: 154, eps: 0.884 rewards = -392.993 loss: 3.815354585647583\n",
      "Episode: 60, Steps: 154, eps: 0.882 rewards = -356.026 loss: 4.214603424072266\n",
      "Episode: 61, Steps: 154, eps: 0.880 rewards = -261.225 loss: 4.13092565536499\n",
      "Episode: 62, Steps: 154, eps: 0.878 rewards = -366.782 loss: 3.671363115310669\n",
      "Episode: 63, Steps: 154, eps: 0.876 rewards = -78.800 loss: 4.006984710693359\n",
      "Episode: 64, Steps: 154, eps: 0.874 rewards = -267.555 loss: 3.3743138313293457\n",
      "Episode: 65, Steps: 154, eps: 0.872 rewards = -64.349 loss: 3.9617433547973633\n",
      "Episode: 66, Steps: 154, eps: 0.870 rewards = -356.952 loss: 2.7925825119018555\n",
      "Episode: 67, Steps: 154, eps: 0.868 rewards = -255.525 loss: 2.9717929363250732\n",
      "Episode: 68, Steps: 154, eps: 0.866 rewards = -308.801 loss: 3.1137430667877197\n",
      "Episode: 69, Steps: 154, eps: 0.864 rewards = -110.112 loss: 1.8441214561462402\n",
      "Episode: 70, Steps: 154, eps: 0.862 rewards = -374.201 loss: 1.7987030744552612\n",
      "Episode: 71, Steps: 154, eps: 0.860 rewards = -299.319 loss: 1.0730412006378174\n",
      "Episode: 72, Steps: 154, eps: 0.858 rewards = -246.166 loss: 1.3660156726837158\n",
      "Episode: 73, Steps: 154, eps: 0.856 rewards = -55.785 loss: 2.322608709335327\n",
      "Episode: 74, Steps: 154, eps: 0.854 rewards = 132.772 loss: 2.199141502380371\n",
      "Episode: 75, Steps: 154, eps: 0.852 rewards = -123.888 loss: 1.4765208959579468\n",
      "Episode: 76, Steps: 154, eps: 0.850 rewards = -375.154 loss: 2.2481632232666016\n",
      "Episode: 77, Steps: 154, eps: 0.848 rewards = -127.665 loss: 1.8182220458984375\n",
      "Episode: 78, Steps: 154, eps: 0.846 rewards = -287.784 loss: 3.032961368560791\n",
      "Episode: 79, Steps: 154, eps: 0.844 rewards = -319.935 loss: 1.7454241514205933\n",
      "Episode: 80, Steps: 154, eps: 0.842 rewards = -306.262 loss: 1.9550193548202515\n",
      "Episode: 81, Steps: 154, eps: 0.840 rewards = -105.745 loss: 2.3803040981292725\n",
      "Episode: 82, Steps: 154, eps: 0.838 rewards = -272.728 loss: 1.3101527690887451\n",
      "Episode: 83, Steps: 154, eps: 0.836 rewards = -170.889 loss: 1.1872317790985107\n",
      "Episode: 84, Steps: 154, eps: 0.834 rewards = -127.395 loss: 1.3541264533996582\n",
      "Episode: 85, Steps: 154, eps: 0.832 rewards = -262.021 loss: 1.1469388008117676\n",
      "Episode: 86, Steps: 154, eps: 0.830 rewards = -108.044 loss: 1.8332160711288452\n",
      "Episode: 87, Steps: 154, eps: 0.828 rewards = -128.784 loss: 1.2669639587402344\n",
      "Episode: 88, Steps: 154, eps: 0.826 rewards = 106.575 loss: 0.2132883369922638\n",
      "Episode: 89, Steps: 46, eps: 0.824 rewards = 25.755 loss: 0.9012778997421265\n",
      "Episode: 90, Steps: 154, eps: 0.822 rewards = 39.920 loss: 1.161350965499878\n",
      "Episode: 91, Steps: 154, eps: 0.820 rewards = -168.683 loss: 1.0976654291152954\n",
      "Episode: 92, Steps: 154, eps: 0.818 rewards = -227.236 loss: 1.2939763069152832\n",
      "Episode: 93, Steps: 154, eps: 0.816 rewards = -160.984 loss: 0.4837048053741455\n",
      "Episode: 94, Steps: 154, eps: 0.814 rewards = -202.670 loss: 0.3079100549221039\n",
      "Episode: 95, Steps: 154, eps: 0.812 rewards = -125.902 loss: 0.41750872135162354\n",
      "Episode: 96, Steps: 154, eps: 0.810 rewards = -200.977 loss: 0.3748388886451721\n",
      "Episode: 97, Steps: 154, eps: 0.808 rewards = -194.129 loss: 0.33942753076553345\n",
      "Episode: 98, Steps: 154, eps: 0.806 rewards = -222.110 loss: 0.4646186828613281\n",
      "Episode: 99, Steps: 154, eps: 0.804 rewards = -190.176 loss: 0.3313952684402466\n",
      "Episode: 100, Steps: 154, eps: 0.802 rewards = -176.104 loss: 0.34348851442337036\n",
      "Episode: 101, Steps: 154, eps: 0.800 rewards = -52.852 loss: 0.8121554851531982\n",
      "Episode: 102, Steps: 154, eps: 0.798 rewards = -126.017 loss: 0.362684428691864\n",
      "Episode: 103, Steps: 154, eps: 0.796 rewards = -194.831 loss: 0.610802173614502\n",
      "Episode: 104, Steps: 154, eps: 0.794 rewards = -115.672 loss: 0.6268236637115479\n",
      "Episode: 105, Steps: 154, eps: 0.792 rewards = -58.412 loss: 0.41533881425857544\n",
      "Episode: 106, Steps: 154, eps: 0.790 rewards = -124.700 loss: 0.5230072736740112\n",
      "Episode: 107, Steps: 154, eps: 0.788 rewards = -247.371 loss: 0.3629135489463806\n",
      "Episode: 108, Steps: 154, eps: 0.786 rewards = -183.176 loss: 0.5628458261489868\n",
      "Episode: 109, Steps: 154, eps: 0.784 rewards = 41.296 loss: 0.4079093635082245\n",
      "Episode: 110, Steps: 154, eps: 0.782 rewards = -124.683 loss: 0.3693854510784149\n",
      "Episode: 111, Steps: 154, eps: 0.780 rewards = -258.521 loss: 0.3129391074180603\n",
      "Episode: 112, Steps: 154, eps: 0.778 rewards = 146.525 loss: 0.5683070421218872\n",
      "Episode: 113, Steps: 154, eps: 0.776 rewards = -96.157 loss: 0.4229549169540405\n",
      "Episode: 114, Steps: 154, eps: 0.774 rewards = -101.540 loss: 0.8321359157562256\n",
      "Episode: 115, Steps: 154, eps: 0.772 rewards = -143.826 loss: 0.3572693169116974\n",
      "Episode: 116, Steps: 154, eps: 0.770 rewards = -253.222 loss: 0.34854912757873535\n",
      "Episode: 117, Steps: 154, eps: 0.768 rewards = -297.452 loss: 0.47783198952674866\n",
      "Episode: 118, Steps: 154, eps: 0.766 rewards = 174.625 loss: 0.3000180721282959\n",
      "Episode: 119, Steps: 154, eps: 0.764 rewards = -242.751 loss: 0.5065657496452332\n",
      "Episode: 120, Steps: 154, eps: 0.762 rewards = -229.747 loss: 0.35503000020980835\n",
      "Episode: 121, Steps: 154, eps: 0.760 rewards = -86.357 loss: 0.6869145631790161\n",
      "Episode: 122, Steps: 154, eps: 0.758 rewards = -98.984 loss: 0.2572973668575287\n",
      "Episode: 123, Steps: 154, eps: 0.756 rewards = 69.293 loss: 0.6022461652755737\n",
      "Episode: 124, Steps: 154, eps: 0.754 rewards = -27.201 loss: 0.5168447494506836\n",
      "Episode: 125, Steps: 48, eps: 0.752 rewards = 75.320 loss: 0.4356120526790619\n",
      "Episode: 126, Steps: 154, eps: 0.750 rewards = 85.280 loss: 0.7514537572860718\n",
      "Episode: 127, Steps: 154, eps: 0.748 rewards = -302.661 loss: 0.25239983201026917\n",
      "Episode: 128, Steps: 71, eps: 0.746 rewards = 149.257 loss: 0.3755062222480774\n",
      "Episode: 129, Steps: 154, eps: 0.744 rewards = 120.489 loss: 0.3436373174190521\n",
      "Episode: 130, Steps: 154, eps: 0.742 rewards = -360.388 loss: 0.8603019714355469\n",
      "Episode: 131, Steps: 154, eps: 0.740 rewards = 176.192 loss: 0.2401997298002243\n",
      "Episode: 132, Steps: 154, eps: 0.738 rewards = 121.069 loss: 0.5119409561157227\n",
      "Episode: 133, Steps: 154, eps: 0.736 rewards = -317.325 loss: 1.0126656293869019\n",
      "Episode: 134, Steps: 154, eps: 0.734 rewards = -159.317 loss: 0.6961524486541748\n",
      "Episode: 135, Steps: 154, eps: 0.732 rewards = 34.537 loss: 0.6072187423706055\n",
      "Episode: 136, Steps: 154, eps: 0.730 rewards = -157.107 loss: 0.9229556322097778\n",
      "Episode: 137, Steps: 154, eps: 0.728 rewards = -194.624 loss: 1.0318019390106201\n",
      "Episode: 138, Steps: 154, eps: 0.726 rewards = -101.644 loss: 0.6555089950561523\n",
      "Episode: 139, Steps: 154, eps: 0.724 rewards = 128.868 loss: 0.3892946243286133\n",
      "Episode: 140, Steps: 154, eps: 0.722 rewards = -27.413 loss: 0.6481446027755737\n",
      "Episode: 141, Steps: 154, eps: 0.720 rewards = -135.340 loss: 0.5153664350509644\n",
      "Episode: 142, Steps: 154, eps: 0.718 rewards = -138.849 loss: 0.508151650428772\n",
      "Episode: 143, Steps: 154, eps: 0.716 rewards = -152.285 loss: 0.6437112092971802\n",
      "Episode: 144, Steps: 154, eps: 0.714 rewards = 61.046 loss: 0.42039936780929565\n",
      "Episode: 145, Steps: 154, eps: 0.712 rewards = 67.115 loss: 0.26088184118270874\n",
      "Episode: 146, Steps: 154, eps: 0.710 rewards = -78.537 loss: 0.15061862766742706\n",
      "Episode: 147, Steps: 154, eps: 0.708 rewards = -109.567 loss: 0.6937849521636963\n",
      "Episode: 148, Steps: 154, eps: 0.706 rewards = -15.119 loss: 0.1808260679244995\n",
      "Episode: 149, Steps: 154, eps: 0.704 rewards = -167.976 loss: 0.5672569870948792\n",
      "Episode: 150, Steps: 154, eps: 0.702 rewards = 5.109 loss: 0.5404890775680542\n",
      "Episode: 151, Steps: 154, eps: 0.700 rewards = -50.862 loss: 0.7004345655441284\n",
      "Episode: 152, Steps: 154, eps: 0.698 rewards = -166.237 loss: 0.6782218217849731\n",
      "Episode: 153, Steps: 154, eps: 0.696 rewards = -182.650 loss: 0.4805487394332886\n",
      "Episode: 154, Steps: 154, eps: 0.694 rewards = -95.254 loss: 0.7998894453048706\n",
      "Episode: 155, Steps: 154, eps: 0.692 rewards = 75.986 loss: 0.5717343091964722\n",
      "Episode: 156, Steps: 154, eps: 0.690 rewards = 118.083 loss: 0.45584890246391296\n",
      "Episode: 157, Steps: 154, eps: 0.688 rewards = 9.158 loss: 0.5311810970306396\n",
      "Episode: 158, Steps: 154, eps: 0.686 rewards = -168.496 loss: 1.2880213260650635\n",
      "Episode: 159, Steps: 154, eps: 0.684 rewards = 24.893 loss: 0.4635840058326721\n",
      "Episode: 160, Steps: 154, eps: 0.682 rewards = -74.140 loss: 0.6335350275039673\n",
      "Episode: 161, Steps: 154, eps: 0.680 rewards = 36.664 loss: 0.974431037902832\n",
      "Episode: 162, Steps: 154, eps: 0.678 rewards = 60.465 loss: 0.5411093235015869\n",
      "Episode: 163, Steps: 154, eps: 0.676 rewards = 34.717 loss: 0.5239742398262024\n",
      "Episode: 164, Steps: 154, eps: 0.674 rewards = 39.210 loss: 0.43523725867271423\n",
      "Episode: 165, Steps: 154, eps: 0.672 rewards = 44.591 loss: 0.6226564645767212\n",
      "Episode: 166, Steps: 154, eps: 0.670 rewards = 120.242 loss: 0.6429601907730103\n",
      "Episode: 167, Steps: 154, eps: 0.668 rewards = -70.728 loss: 0.6019928455352783\n",
      "Episode: 168, Steps: 154, eps: 0.666 rewards = -78.272 loss: 0.43606215715408325\n",
      "Episode: 169, Steps: 154, eps: 0.664 rewards = -5.728 loss: 0.7085477113723755\n",
      "Episode: 170, Steps: 154, eps: 0.662 rewards = -74.376 loss: 0.5484223365783691\n",
      "Episode: 171, Steps: 154, eps: 0.660 rewards = -14.814 loss: 0.5330889225006104\n",
      "Episode: 172, Steps: 154, eps: 0.658 rewards = -207.273 loss: 0.9855207800865173\n",
      "Episode: 173, Steps: 154, eps: 0.656 rewards = 200.026 loss: 0.23372556269168854\n",
      "Episode: 174, Steps: 154, eps: 0.654 rewards = -110.454 loss: 0.33093544840812683\n",
      "Episode: 175, Steps: 154, eps: 0.652 rewards = 11.697 loss: 0.7324415445327759\n",
      "Episode: 176, Steps: 154, eps: 0.650 rewards = 9.597 loss: 0.5109096765518188\n",
      "Episode: 177, Steps: 154, eps: 0.648 rewards = 20.520 loss: 0.39522111415863037\n",
      "Episode: 178, Steps: 154, eps: 0.646 rewards = 5.495 loss: 0.7601079344749451\n",
      "Episode: 179, Steps: 154, eps: 0.644 rewards = -14.600 loss: 0.48638415336608887\n",
      "Episode: 180, Steps: 154, eps: 0.642 rewards = -23.566 loss: 0.3198176920413971\n",
      "Episode: 181, Steps: 154, eps: 0.640 rewards = -34.666 loss: 0.3986913561820984\n",
      "Episode: 182, Steps: 154, eps: 0.638 rewards = 148.326 loss: 0.26091617345809937\n",
      "Episode: 183, Steps: 154, eps: 0.636 rewards = 37.158 loss: 0.442478209733963\n",
      "Episode: 184, Steps: 154, eps: 0.634 rewards = 5.544 loss: 0.3752296268939972\n",
      "Episode: 185, Steps: 154, eps: 0.632 rewards = 50.044 loss: 0.46427029371261597\n",
      "Episode: 186, Steps: 154, eps: 0.630 rewards = 18.565 loss: 0.2680427134037018\n",
      "Episode: 187, Steps: 154, eps: 0.628 rewards = 113.347 loss: 0.39904293417930603\n",
      "Episode: 188, Steps: 154, eps: 0.626 rewards = -104.089 loss: 0.873505711555481\n",
      "Episode: 189, Steps: 154, eps: 0.624 rewards = -88.707 loss: 0.8281506299972534\n",
      "Episode: 190, Steps: 154, eps: 0.622 rewards = -48.849 loss: 0.4688493609428406\n",
      "Episode: 191, Steps: 154, eps: 0.620 rewards = 123.014 loss: 0.6420589685440063\n",
      "Episode: 192, Steps: 154, eps: 0.618 rewards = -60.297 loss: 0.7360810041427612\n",
      "Episode: 193, Steps: 154, eps: 0.616 rewards = -38.753 loss: 0.9914908409118652\n",
      "Episode: 194, Steps: 154, eps: 0.614 rewards = -8.237 loss: 0.3594721555709839\n",
      "Episode: 195, Steps: 154, eps: 0.612 rewards = -17.774 loss: 0.4932904541492462\n",
      "Episode: 196, Steps: 154, eps: 0.610 rewards = 7.350 loss: 0.450368195772171\n",
      "Episode: 197, Steps: 154, eps: 0.608 rewards = 18.858 loss: 0.35433298349380493\n",
      "Episode: 198, Steps: 154, eps: 0.606 rewards = 27.157 loss: 0.8973823189735413\n",
      "Episode: 199, Steps: 154, eps: 0.604 rewards = 110.841 loss: 0.44364479184150696\n",
      "Episode: 200, Steps: 154, eps: 0.602 rewards = -59.137 loss: 1.223659634590149\n",
      "Episode: 201, Steps: 154, eps: 0.600 rewards = 0.105 loss: 0.22717233002185822\n",
      "Episode: 202, Steps: 154, eps: 0.598 rewards = 44.496 loss: 0.4013243615627289\n",
      "Episode: 203, Steps: 154, eps: 0.596 rewards = -56.031 loss: 0.1812456101179123\n",
      "Episode: 204, Steps: 154, eps: 0.594 rewards = 23.887 loss: 0.19380469620227814\n",
      "Episode: 205, Steps: 154, eps: 0.592 rewards = -5.101 loss: 0.24831128120422363\n",
      "Episode: 206, Steps: 154, eps: 0.590 rewards = -22.491 loss: 0.21895670890808105\n",
      "Episode: 207, Steps: 154, eps: 0.588 rewards = -4.834 loss: 0.2048487514257431\n",
      "Episode: 208, Steps: 154, eps: 0.586 rewards = 11.409 loss: 0.13969460129737854\n",
      "Episode: 209, Steps: 154, eps: 0.584 rewards = -11.403 loss: 0.12158218026161194\n",
      "Episode: 210, Steps: 154, eps: 0.582 rewards = -27.989 loss: 0.17893345654010773\n",
      "Episode: 211, Steps: 154, eps: 0.580 rewards = 17.590 loss: 0.6903754472732544\n",
      "Episode: 212, Steps: 154, eps: 0.578 rewards = -53.501 loss: 0.2422417551279068\n",
      "Episode: 213, Steps: 154, eps: 0.576 rewards = 48.644 loss: 0.11966095864772797\n",
      "Episode: 214, Steps: 154, eps: 0.574 rewards = -4.993 loss: 0.14669454097747803\n",
      "Episode: 215, Steps: 154, eps: 0.572 rewards = 9.204 loss: 0.33552682399749756\n",
      "Episode: 216, Steps: 154, eps: 0.570 rewards = -56.425 loss: 0.23542234301567078\n",
      "Episode: 217, Steps: 154, eps: 0.568 rewards = -47.669 loss: 0.5728766918182373\n",
      "Episode: 218, Steps: 154, eps: 0.566 rewards = -28.380 loss: 0.6844996213912964\n",
      "Episode: 219, Steps: 154, eps: 0.564 rewards = 78.935 loss: 0.44580593705177307\n",
      "Episode: 220, Steps: 154, eps: 0.562 rewards = -95.263 loss: 0.5378034114837646\n",
      "Episode: 221, Steps: 154, eps: 0.560 rewards = 23.263 loss: 0.7922252416610718\n",
      "Episode: 222, Steps: 154, eps: 0.558 rewards = -63.250 loss: 1.1007771492004395\n",
      "Episode: 223, Steps: 154, eps: 0.556 rewards = -9.360 loss: 0.7244389057159424\n",
      "Episode: 224, Steps: 154, eps: 0.554 rewards = -87.920 loss: 0.7569820880889893\n",
      "Episode: 225, Steps: 154, eps: 0.552 rewards = 115.328 loss: 0.593748927116394\n",
      "Episode: 226, Steps: 154, eps: 0.550 rewards = -0.124 loss: 0.37489426136016846\n",
      "Episode: 227, Steps: 154, eps: 0.548 rewards = -57.002 loss: 0.9545021057128906\n",
      "Episode: 228, Steps: 154, eps: 0.546 rewards = 18.606 loss: 0.6822753548622131\n",
      "Episode: 229, Steps: 154, eps: 0.544 rewards = -4.695 loss: 0.6686348915100098\n",
      "Episode: 230, Steps: 154, eps: 0.542 rewards = -66.580 loss: 0.5154696702957153\n",
      "Episode: 231, Steps: 154, eps: 0.540 rewards = -73.887 loss: 0.5689340829849243\n",
      "Episode: 232, Steps: 154, eps: 0.538 rewards = -97.245 loss: 0.40712690353393555\n",
      "Episode: 233, Steps: 154, eps: 0.536 rewards = -25.674 loss: 0.1889411062002182\n",
      "Episode: 234, Steps: 154, eps: 0.534 rewards = 3.247 loss: 0.22389893233776093\n",
      "Episode: 235, Steps: 154, eps: 0.532 rewards = -150.726 loss: 0.4393277168273926\n",
      "Episode: 236, Steps: 154, eps: 0.530 rewards = -65.692 loss: 0.38248422741889954\n",
      "Episode: 237, Steps: 154, eps: 0.528 rewards = -96.348 loss: 0.40919968485832214\n",
      "Episode: 238, Steps: 154, eps: 0.526 rewards = 1.352 loss: 0.19587190449237823\n",
      "Episode: 239, Steps: 154, eps: 0.524 rewards = -168.808 loss: 0.4487924873828888\n",
      "Episode: 240, Steps: 154, eps: 0.522 rewards = -122.208 loss: 0.445041686296463\n",
      "Episode: 241, Steps: 154, eps: 0.520 rewards = -127.296 loss: 0.480588436126709\n",
      "Episode: 242, Steps: 154, eps: 0.518 rewards = -73.185 loss: 0.2442937195301056\n",
      "Episode: 243, Steps: 154, eps: 0.516 rewards = -9.433 loss: 0.2856845259666443\n",
      "Episode: 244, Steps: 154, eps: 0.514 rewards = 98.220 loss: 0.283081978559494\n",
      "Episode: 245, Steps: 154, eps: 0.512 rewards = -42.047 loss: 0.2974405884742737\n",
      "Episode: 246, Steps: 154, eps: 0.510 rewards = -144.892 loss: 0.4237402677536011\n",
      "Episode: 247, Steps: 154, eps: 0.508 rewards = -1.110 loss: 0.22622427344322205\n",
      "Episode: 248, Steps: 154, eps: 0.506 rewards = 147.143 loss: 0.2687036991119385\n",
      "Episode: 249, Steps: 154, eps: 0.504 rewards = 1.753 loss: 0.32824230194091797\n",
      "Episode: 250, Steps: 154, eps: 0.502 rewards = -55.515 loss: 0.21402709186077118\n",
      "Episode: 251, Steps: 154, eps: 0.500 rewards = 168.368 loss: 0.20945723354816437\n",
      "Episode: 252, Steps: 154, eps: 0.498 rewards = 131.895 loss: 0.544076681137085\n",
      "Episode: 253, Steps: 154, eps: 0.496 rewards = -55.520 loss: 0.3458763062953949\n",
      "Episode: 254, Steps: 154, eps: 0.494 rewards = -127.370 loss: 0.3558424711227417\n",
      "Episode: 255, Steps: 154, eps: 0.492 rewards = -5.871 loss: 0.42960721254348755\n",
      "Episode: 256, Steps: 154, eps: 0.490 rewards = 30.534 loss: 0.4292280375957489\n",
      "Episode: 257, Steps: 154, eps: 0.488 rewards = -79.185 loss: 0.23052671551704407\n",
      "Episode: 258, Steps: 154, eps: 0.486 rewards = 51.913 loss: 0.3188905715942383\n",
      "Episode: 259, Steps: 154, eps: 0.484 rewards = -72.863 loss: 0.3632551431655884\n",
      "Episode: 260, Steps: 154, eps: 0.482 rewards = 72.036 loss: 0.3332486152648926\n",
      "Episode: 261, Steps: 154, eps: 0.480 rewards = -34.237 loss: 0.20477941632270813\n",
      "Episode: 262, Steps: 154, eps: 0.478 rewards = -57.315 loss: 0.3098680078983307\n",
      "Episode: 263, Steps: 154, eps: 0.476 rewards = 35.114 loss: 0.4649573862552643\n",
      "Episode: 264, Steps: 154, eps: 0.474 rewards = -61.715 loss: 0.19083334505558014\n",
      "Episode: 265, Steps: 154, eps: 0.472 rewards = 155.990 loss: 0.35107097029685974\n",
      "Episode: 266, Steps: 154, eps: 0.470 rewards = 20.961 loss: 0.25385716557502747\n",
      "Episode: 267, Steps: 154, eps: 0.468 rewards = -89.932 loss: 0.21347492933273315\n",
      "Episode: 268, Steps: 154, eps: 0.466 rewards = -40.186 loss: 0.25383007526397705\n",
      "Episode: 269, Steps: 154, eps: 0.464 rewards = -386.809 loss: 0.3424445688724518\n",
      "Episode: 270, Steps: 154, eps: 0.462 rewards = 83.794 loss: 0.21662870049476624\n",
      "Episode: 271, Steps: 154, eps: 0.460 rewards = -34.782 loss: 0.6342270374298096\n",
      "Episode: 272, Steps: 154, eps: 0.458 rewards = -78.697 loss: 0.43474632501602173\n",
      "Episode: 273, Steps: 154, eps: 0.456 rewards = 10.236 loss: 0.3884222209453583\n",
      "Episode: 274, Steps: 154, eps: 0.454 rewards = 16.719 loss: 0.4877692461013794\n",
      "Episode: 275, Steps: 154, eps: 0.452 rewards = 7.348 loss: 1.0079667568206787\n",
      "Episode: 276, Steps: 154, eps: 0.450 rewards = 36.752 loss: 0.3205820918083191\n",
      "Episode: 277, Steps: 154, eps: 0.448 rewards = 35.842 loss: 0.20180357992649078\n",
      "Episode: 278, Steps: 154, eps: 0.446 rewards = 54.653 loss: 0.1811213195323944\n",
      "Episode: 279, Steps: 154, eps: 0.444 rewards = 4.192 loss: 0.24759066104888916\n",
      "Episode: 280, Steps: 154, eps: 0.442 rewards = 1.597 loss: 0.22989250719547272\n",
      "Episode: 281, Steps: 154, eps: 0.440 rewards = -44.018 loss: 0.28166404366493225\n",
      "Episode: 282, Steps: 154, eps: 0.438 rewards = 90.010 loss: 0.1557193398475647\n",
      "Episode: 283, Steps: 154, eps: 0.436 rewards = -52.260 loss: 0.5566619634628296\n",
      "Episode: 284, Steps: 154, eps: 0.434 rewards = -2.022 loss: 0.4844927489757538\n",
      "Episode: 285, Steps: 154, eps: 0.432 rewards = 92.986 loss: 0.6469385027885437\n",
      "Episode: 286, Steps: 154, eps: 0.430 rewards = -32.968 loss: 0.14956609904766083\n",
      "Episode: 287, Steps: 154, eps: 0.428 rewards = 63.096 loss: 0.31324470043182373\n",
      "Episode: 288, Steps: 154, eps: 0.426 rewards = -58.497 loss: 0.4126436710357666\n",
      "Episode: 289, Steps: 154, eps: 0.424 rewards = -12.362 loss: 0.31216365098953247\n",
      "Episode: 290, Steps: 154, eps: 0.422 rewards = -28.837 loss: 0.11679749935865402\n",
      "Episode: 291, Steps: 154, eps: 0.420 rewards = 93.391 loss: 0.227390319108963\n",
      "Episode: 292, Steps: 154, eps: 0.418 rewards = -84.330 loss: 0.2947026193141937\n",
      "Episode: 293, Steps: 154, eps: 0.416 rewards = 46.209 loss: 0.5132036805152893\n",
      "Episode: 294, Steps: 154, eps: 0.414 rewards = -90.732 loss: 0.6965047717094421\n",
      "Episode: 295, Steps: 154, eps: 0.412 rewards = 12.065 loss: 0.26587212085723877\n",
      "Episode: 296, Steps: 154, eps: 0.410 rewards = -100.727 loss: 0.23275531828403473\n",
      "Episode: 297, Steps: 154, eps: 0.408 rewards = 62.806 loss: 0.4058104157447815\n",
      "Episode: 298, Steps: 154, eps: 0.406 rewards = -50.134 loss: 0.26423558592796326\n",
      "Episode: 299, Steps: 154, eps: 0.404 rewards = -60.613 loss: 0.32545018196105957\n",
      "Episode: 300, Steps: 154, eps: 0.402 rewards = 4.222 loss: 0.45503950119018555\n",
      "Episode: 301, Steps: 154, eps: 0.400 rewards = 6.689 loss: 0.23205867409706116\n",
      "Episode: 302, Steps: 154, eps: 0.398 rewards = -30.919 loss: 0.33631688356399536\n",
      "Episode: 303, Steps: 154, eps: 0.396 rewards = 7.699 loss: 0.4868454337120056\n",
      "Episode: 304, Steps: 154, eps: 0.394 rewards = -21.293 loss: 0.4241738021373749\n",
      "Episode: 305, Steps: 154, eps: 0.392 rewards = 38.615 loss: 0.2805019021034241\n",
      "Episode: 306, Steps: 154, eps: 0.390 rewards = -67.078 loss: 0.26102933287620544\n",
      "Episode: 307, Steps: 154, eps: 0.388 rewards = 57.953 loss: 0.10566699504852295\n",
      "Episode: 308, Steps: 154, eps: 0.386 rewards = 149.863 loss: 0.12469065934419632\n",
      "Episode: 309, Steps: 154, eps: 0.384 rewards = 83.508 loss: 0.4505503475666046\n",
      "Episode: 310, Steps: 154, eps: 0.382 rewards = 39.878 loss: 0.15695810317993164\n",
      "Episode: 311, Steps: 154, eps: 0.380 rewards = -41.757 loss: 0.2409229576587677\n",
      "Episode: 312, Steps: 154, eps: 0.378 rewards = 163.557 loss: 0.4591233730316162\n",
      "Episode: 313, Steps: 154, eps: 0.376 rewards = 40.714 loss: 0.6278438568115234\n",
      "Episode: 314, Steps: 154, eps: 0.374 rewards = 82.586 loss: 0.21964266896247864\n",
      "Episode: 315, Steps: 154, eps: 0.372 rewards = 44.685 loss: 0.22857514023780823\n",
      "Episode: 316, Steps: 154, eps: 0.370 rewards = 17.977 loss: 0.22301113605499268\n",
      "Episode: 317, Steps: 154, eps: 0.368 rewards = -61.176 loss: 0.5396251082420349\n",
      "Episode: 318, Steps: 154, eps: 0.366 rewards = 73.736 loss: 0.1752493530511856\n",
      "Episode: 319, Steps: 154, eps: 0.364 rewards = 60.044 loss: 0.16766388714313507\n",
      "Episode: 320, Steps: 154, eps: 0.362 rewards = 125.680 loss: 0.2644110918045044\n",
      "Episode: 321, Steps: 154, eps: 0.360 rewards = -29.528 loss: 0.11585825681686401\n",
      "Episode: 322, Steps: 154, eps: 0.358 rewards = -18.689 loss: 0.35455232858657837\n",
      "Episode: 323, Steps: 154, eps: 0.356 rewards = -7.884 loss: 0.2635973393917084\n",
      "Episode: 324, Steps: 154, eps: 0.354 rewards = -7.310 loss: 0.293856143951416\n",
      "Episode: 325, Steps: 154, eps: 0.352 rewards = 62.170 loss: 0.26103299856185913\n",
      "Episode: 326, Steps: 154, eps: 0.350 rewards = -5.719 loss: 0.3801092803478241\n",
      "Episode: 327, Steps: 154, eps: 0.348 rewards = -77.446 loss: 0.2575231194496155\n",
      "Episode: 328, Steps: 154, eps: 0.346 rewards = 4.122 loss: 0.5289711356163025\n",
      "Episode: 329, Steps: 154, eps: 0.344 rewards = 5.702 loss: 0.1067722737789154\n",
      "Episode: 330, Steps: 154, eps: 0.342 rewards = 24.346 loss: 0.30357107520103455\n",
      "Episode: 331, Steps: 154, eps: 0.340 rewards = -82.011 loss: 0.5610108375549316\n",
      "Episode: 332, Steps: 154, eps: 0.338 rewards = -108.577 loss: 0.4851887822151184\n",
      "Episode: 333, Steps: 154, eps: 0.336 rewards = -98.134 loss: 0.6577154397964478\n",
      "Episode: 334, Steps: 154, eps: 0.334 rewards = -3.790 loss: 0.3182615637779236\n",
      "Episode: 335, Steps: 154, eps: 0.332 rewards = -23.433 loss: 0.39847320318222046\n",
      "Episode: 336, Steps: 154, eps: 0.330 rewards = 44.696 loss: 0.5071953535079956\n",
      "Episode: 337, Steps: 154, eps: 0.328 rewards = 134.276 loss: 0.7067843675613403\n",
      "Episode: 338, Steps: 154, eps: 0.326 rewards = 12.571 loss: 0.22871840000152588\n",
      "Episode: 339, Steps: 154, eps: 0.324 rewards = 53.367 loss: 0.2807422876358032\n",
      "Episode: 340, Steps: 154, eps: 0.322 rewards = 85.731 loss: 0.16865870356559753\n",
      "Episode: 341, Steps: 154, eps: 0.320 rewards = -56.424 loss: 0.21395182609558105\n",
      "Episode: 342, Steps: 154, eps: 0.318 rewards = 16.111 loss: 0.45438718795776367\n",
      "Episode: 343, Steps: 154, eps: 0.316 rewards = -41.139 loss: 0.3348890542984009\n",
      "Episode: 344, Steps: 154, eps: 0.314 rewards = -51.877 loss: 0.15851664543151855\n",
      "Episode: 345, Steps: 154, eps: 0.312 rewards = -72.428 loss: 0.30858319997787476\n",
      "Episode: 346, Steps: 154, eps: 0.310 rewards = 179.069 loss: 0.490069717168808\n",
      "Episode: 347, Steps: 154, eps: 0.308 rewards = -9.320 loss: 0.44081637263298035\n",
      "Episode: 348, Steps: 154, eps: 0.306 rewards = 121.819 loss: 0.9120165109634399\n",
      "Episode: 349, Steps: 154, eps: 0.304 rewards = 53.387 loss: 1.391706943511963\n",
      "Episode: 350, Steps: 154, eps: 0.302 rewards = 40.068 loss: 0.34143689274787903\n",
      "Episode: 351, Steps: 154, eps: 0.300 rewards = 31.666 loss: 0.26157256960868835\n",
      "Episode: 352, Steps: 154, eps: 0.298 rewards = 43.530 loss: 0.11198383569717407\n",
      "Episode: 353, Steps: 154, eps: 0.296 rewards = -46.697 loss: 0.17799246311187744\n",
      "Episode: 354, Steps: 154, eps: 0.294 rewards = -104.519 loss: 0.5455148816108704\n",
      "Episode: 355, Steps: 154, eps: 0.292 rewards = -74.301 loss: 0.3220359981060028\n",
      "Episode: 356, Steps: 154, eps: 0.290 rewards = 56.497 loss: 0.5193989276885986\n",
      "Episode: 357, Steps: 154, eps: 0.288 rewards = -8.718 loss: 0.39181509613990784\n",
      "Episode: 358, Steps: 154, eps: 0.286 rewards = -27.495 loss: 0.42950767278671265\n",
      "Episode: 359, Steps: 154, eps: 0.284 rewards = -9.859 loss: 0.16177980601787567\n",
      "Episode: 360, Steps: 154, eps: 0.282 rewards = -8.893 loss: 0.22043979167938232\n",
      "Episode: 361, Steps: 154, eps: 0.280 rewards = -56.898 loss: 0.16190779209136963\n",
      "Episode: 362, Steps: 154, eps: 0.278 rewards = -68.636 loss: 0.22183164954185486\n",
      "Episode: 363, Steps: 154, eps: 0.276 rewards = -46.910 loss: 0.35596930980682373\n",
      "Episode: 364, Steps: 154, eps: 0.274 rewards = -50.218 loss: 0.19959081709384918\n",
      "Episode: 365, Steps: 154, eps: 0.272 rewards = 150.297 loss: 0.21115022897720337\n",
      "Episode: 366, Steps: 154, eps: 0.270 rewards = 47.436 loss: 0.17430788278579712\n",
      "Episode: 367, Steps: 154, eps: 0.268 rewards = 75.593 loss: 0.15096145868301392\n",
      "Episode: 368, Steps: 154, eps: 0.266 rewards = -58.608 loss: 0.12160269916057587\n",
      "Episode: 369, Steps: 154, eps: 0.264 rewards = 9.955 loss: 0.17949111759662628\n",
      "Episode: 370, Steps: 154, eps: 0.262 rewards = 62.795 loss: 0.20987829566001892\n",
      "Episode: 371, Steps: 154, eps: 0.260 rewards = 90.698 loss: 0.14953312277793884\n",
      "Episode: 372, Steps: 154, eps: 0.258 rewards = 6.168 loss: 0.3584466278553009\n",
      "Episode: 373, Steps: 154, eps: 0.256 rewards = -42.333 loss: 0.26175016164779663\n",
      "Episode: 374, Steps: 154, eps: 0.254 rewards = 10.396 loss: 0.1347745954990387\n",
      "Episode: 375, Steps: 154, eps: 0.252 rewards = -41.749 loss: 0.19201979041099548\n",
      "Episode: 376, Steps: 154, eps: 0.250 rewards = -3.509 loss: 0.23295480012893677\n",
      "Episode: 377, Steps: 154, eps: 0.248 rewards = 148.208 loss: 0.2843206822872162\n",
      "Episode: 378, Steps: 154, eps: 0.246 rewards = 40.143 loss: 0.14922739565372467\n",
      "Episode: 379, Steps: 154, eps: 0.244 rewards = 53.719 loss: 0.1354166865348816\n",
      "Episode: 380, Steps: 154, eps: 0.242 rewards = 160.001 loss: 0.3395392596721649\n",
      "Episode: 381, Steps: 154, eps: 0.240 rewards = 118.384 loss: 0.48855483531951904\n",
      "Episode: 382, Steps: 154, eps: 0.238 rewards = 29.872 loss: 0.43183276057243347\n",
      "Episode: 383, Steps: 154, eps: 0.236 rewards = 69.458 loss: 0.5461583733558655\n",
      "Episode: 384, Steps: 154, eps: 0.234 rewards = 26.969 loss: 0.5528920292854309\n",
      "Episode: 385, Steps: 154, eps: 0.232 rewards = 100.708 loss: 0.32415974140167236\n",
      "Episode: 386, Steps: 154, eps: 0.230 rewards = 14.210 loss: 0.23053553700447083\n",
      "Episode: 387, Steps: 154, eps: 0.228 rewards = -35.175 loss: 0.44013068079948425\n",
      "Episode: 388, Steps: 154, eps: 0.226 rewards = 67.565 loss: 0.4916706085205078\n",
      "Episode: 389, Steps: 154, eps: 0.224 rewards = -0.908 loss: 0.5301507711410522\n",
      "Episode: 390, Steps: 154, eps: 0.222 rewards = -33.480 loss: 0.37305378913879395\n",
      "Episode: 391, Steps: 154, eps: 0.220 rewards = 40.769 loss: 0.3133264183998108\n",
      "Episode: 392, Steps: 154, eps: 0.218 rewards = 0.106 loss: 0.4229086935520172\n",
      "Episode: 393, Steps: 154, eps: 0.216 rewards = 104.088 loss: 0.37079572677612305\n",
      "Episode: 394, Steps: 154, eps: 0.214 rewards = -12.085 loss: 0.4430124759674072\n",
      "Episode: 395, Steps: 154, eps: 0.212 rewards = 168.140 loss: 0.21645522117614746\n",
      "Episode: 396, Steps: 154, eps: 0.210 rewards = 37.009 loss: 0.5276971459388733\n",
      "Episode: 397, Steps: 154, eps: 0.208 rewards = 173.558 loss: 0.44609957933425903\n",
      "Episode: 398, Steps: 154, eps: 0.206 rewards = 75.950 loss: 0.2748227119445801\n",
      "Episode: 399, Steps: 154, eps: 0.204 rewards = 54.804 loss: 0.370849609375\n",
      "Episode: 400, Steps: 154, eps: 0.202 rewards = -42.598 loss: 0.3251335620880127\n",
      "Episode: 401, Steps: 154, eps: 0.200 rewards = 5.644 loss: 0.4996742904186249\n",
      "Episode: 402, Steps: 154, eps: 0.198 rewards = 94.092 loss: 0.6169483661651611\n",
      "Episode: 403, Steps: 154, eps: 0.196 rewards = 149.030 loss: 0.3626202940940857\n",
      "Episode: 404, Steps: 154, eps: 0.194 rewards = 0.662 loss: 0.20781753957271576\n",
      "Episode: 405, Steps: 154, eps: 0.192 rewards = 33.046 loss: 0.20005741715431213\n",
      "Episode: 406, Steps: 154, eps: 0.190 rewards = 26.574 loss: 0.2609030604362488\n",
      "Episode: 407, Steps: 154, eps: 0.188 rewards = 5.931 loss: 0.4023587107658386\n",
      "Episode: 408, Steps: 154, eps: 0.186 rewards = 93.747 loss: 0.2476576566696167\n",
      "Episode: 409, Steps: 154, eps: 0.184 rewards = 0.418 loss: 0.5400171875953674\n",
      "Episode: 410, Steps: 154, eps: 0.182 rewards = 9.139 loss: 0.18002763390541077\n",
      "Episode: 411, Steps: 154, eps: 0.180 rewards = 81.364 loss: 0.3349642753601074\n",
      "Episode: 412, Steps: 154, eps: 0.178 rewards = 66.456 loss: 0.08646126091480255\n",
      "Episode: 413, Steps: 154, eps: 0.176 rewards = 64.186 loss: 0.05492335557937622\n",
      "Episode: 414, Steps: 154, eps: 0.174 rewards = 64.582 loss: 0.280905157327652\n",
      "Episode: 415, Steps: 154, eps: 0.172 rewards = 134.758 loss: 0.28981661796569824\n",
      "Episode: 416, Steps: 154, eps: 0.170 rewards = 103.672 loss: 0.26449933648109436\n",
      "Episode: 417, Steps: 154, eps: 0.168 rewards = 65.777 loss: 0.2880971431732178\n",
      "Episode: 418, Steps: 154, eps: 0.166 rewards = 47.849 loss: 0.3240949809551239\n",
      "Episode: 419, Steps: 154, eps: 0.164 rewards = -30.826 loss: 0.4951104521751404\n",
      "Episode: 420, Steps: 154, eps: 0.162 rewards = 54.029 loss: 0.3418174386024475\n",
      "Episode: 421, Steps: 154, eps: 0.160 rewards = 129.173 loss: 0.29274797439575195\n",
      "Episode: 422, Steps: 154, eps: 0.158 rewards = 1.716 loss: 0.19958283007144928\n",
      "Episode: 423, Steps: 154, eps: 0.156 rewards = 157.230 loss: 0.35661637783050537\n",
      "Episode: 424, Steps: 154, eps: 0.154 rewards = 29.506 loss: 0.2551455497741699\n",
      "Episode: 425, Steps: 154, eps: 0.152 rewards = 34.304 loss: 0.1592147797346115\n",
      "Episode: 426, Steps: 154, eps: 0.150 rewards = 47.002 loss: 0.6811122894287109\n",
      "Episode: 427, Steps: 154, eps: 0.148 rewards = -33.000 loss: 0.21945832669734955\n",
      "Episode: 428, Steps: 96, eps: 0.146 rewards = 124.746 loss: 0.40892577171325684\n",
      "Episode: 429, Steps: 154, eps: 0.144 rewards = 10.815 loss: 0.7922767996788025\n",
      "Episode: 430, Steps: 154, eps: 0.142 rewards = 201.753 loss: 0.32906898856163025\n",
      "Episode: 431, Steps: 154, eps: 0.140 rewards = -49.730 loss: 0.4112461507320404\n",
      "Episode: 432, Steps: 154, eps: 0.138 rewards = -27.815 loss: 0.16888181865215302\n",
      "Episode: 433, Steps: 154, eps: 0.136 rewards = -6.630 loss: 0.35846513509750366\n",
      "Episode: 434, Steps: 154, eps: 0.134 rewards = 44.942 loss: 0.6000430583953857\n",
      "Episode: 435, Steps: 154, eps: 0.132 rewards = 47.820 loss: 0.8623393774032593\n",
      "Episode: 436, Steps: 154, eps: 0.130 rewards = 1.886 loss: 0.2528475821018219\n",
      "Episode: 437, Steps: 154, eps: 0.128 rewards = 79.203 loss: 0.5073000192642212\n",
      "Episode: 438, Steps: 154, eps: 0.126 rewards = 65.620 loss: 0.4886224865913391\n",
      "Episode: 439, Steps: 154, eps: 0.124 rewards = 13.134 loss: 0.31218376755714417\n",
      "Episode: 440, Steps: 154, eps: 0.122 rewards = 145.540 loss: 0.4009154438972473\n",
      "Episode: 441, Steps: 154, eps: 0.120 rewards = 43.963 loss: 0.5576996207237244\n",
      "Episode: 442, Steps: 154, eps: 0.118 rewards = 9.184 loss: 0.19925205409526825\n",
      "Episode: 443, Steps: 154, eps: 0.116 rewards = 9.643 loss: 0.37161314487457275\n",
      "Episode: 444, Steps: 154, eps: 0.114 rewards = 97.056 loss: 0.27612513303756714\n",
      "Episode: 445, Steps: 154, eps: 0.112 rewards = 31.547 loss: 0.36524665355682373\n",
      "Episode: 446, Steps: 154, eps: 0.110 rewards = -54.229 loss: 0.4181175231933594\n",
      "Episode: 447, Steps: 154, eps: 0.108 rewards = 137.817 loss: 0.563279390335083\n",
      "Episode: 448, Steps: 154, eps: 0.106 rewards = 137.719 loss: 0.1805366575717926\n",
      "Episode: 449, Steps: 154, eps: 0.104 rewards = 1.880 loss: 0.3514545261859894\n",
      "Episode: 450, Steps: 154, eps: 0.102 rewards = -2.502 loss: 0.18407505750656128\n",
      "Episode: 451, Steps: 154, eps: 0.100 rewards = -59.721 loss: 0.2987597584724426\n",
      "Episode: 452, Steps: 154, eps: 0.098 rewards = 44.776 loss: 0.21378237009048462\n",
      "Episode: 453, Steps: 154, eps: 0.096 rewards = -176.384 loss: 0.7035282850265503\n",
      "Episode: 454, Steps: 27, eps: 0.094 rewards = 27.188 loss: 0.520818293094635\n",
      "Episode: 455, Steps: 154, eps: 0.092 rewards = 61.672 loss: 0.5048409104347229\n",
      "Episode: 456, Steps: 154, eps: 0.090 rewards = -49.897 loss: 0.16688095033168793\n",
      "Episode: 457, Steps: 132, eps: 0.088 rewards = 208.435 loss: 0.38956308364868164\n",
      "Episode: 458, Steps: 154, eps: 0.086 rewards = 106.910 loss: 0.16975867748260498\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m      8\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m episode \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monline_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     old_obs, act, reward, obs, term, trunc \u001b[38;5;241m=\u001b[39m replay_buffer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     12\u001b[0m     rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[29], line 9\u001b[0m, in \u001b[0;36mplay_one_step\u001b[1;34m(env, model, obs, epsilon, replay_buffer)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_one_step\u001b[39m(env, model, obs, epsilon, replay_buffer):\n\u001b[1;32m----> 9\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mepsilon_greedy_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprisoner\u001b[39m\u001b[38;5;124m\"\u001b[39m: action}\n\u001b[0;32m     11\u001b[0m     next_obs, reward, term, trunc, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m, in \u001b[0;36mepsilon_greedy_policy\u001b[1;34m(model, state, epsilon)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     Q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(Q_values)\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:505\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    503\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menumerate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_predict_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:649\u001b[0m, in \u001b[0;36mTFEpochIterator.enumerate_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 649\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distributed_dataset)\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution\n\u001b[0;32m    653\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    701\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    704\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 705\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    741\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    742\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    743\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 744\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for episode in range(600):\n",
    "    obs, info = env.reset()\n",
    "    obs = obs[\"prisoner\"] / 10\n",
    "    rewards = 0\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        play_one_step(env, online_model, obs, epsilon, replay_buffer)\n",
    "\n",
    "        old_obs, act, reward, obs, term, trunc = replay_buffer[-1]\n",
    "        rewards += reward\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    print(f\"\\rEpisode: {episode+1}, Steps: {step+1}, eps: {epsilon:.3f} {rewards = :.3f} \", end=\"\")\n",
    "\n",
    "    if episode > 50:\n",
    "        training_step(online_model, target_model, replay_buffer, optimizer,\n",
    "                    loss_fn, batch_size, discount_factor)\n",
    "\n",
    "    # rewards.append(step)\n",
    "    # if step >= best_score:\n",
    "    #     best_weights = model.get_weights()\n",
    "    #     best_score = step\n",
    "\n",
    "# model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def one_step(env, obs, model):\n",
    "    prisoner_inputs = tf.convert_to_tensor([obs[\"prisoner\"]])\n",
    "    output = model.predict(prisoner_inputs, verbose=0)\n",
    "    predicted_action = int(tf.argmax(output, axis=1))\n",
    "\n",
    "    actions = {\"prisoner\": predicted_action}\n",
    "    print(f\"\\r{actions = } \", end=\"\")\n",
    "    obs, rewards, term, trunc, infos = env.step(actions)\n",
    "    return obs, rewards, term, trunc\n",
    "\n",
    "def play(env, model, loss_fn):\n",
    "    obs, info = env.reset()\n",
    "    # obs, info = env.reset(seed=24)\n",
    "    rewards_list, grads_list = [], []\n",
    "\n",
    "    while env.agents:\n",
    "        obs, rew, term, trunc = one_step(env, obs, model)\n",
    "        rewards_list.append(rew[\"prisoner\"])\n",
    "        print(f\"{rew = }\", end=\"\")\n",
    "    return rewards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions = {'prisoner': 3} rew = {'prisoner': -1.54937744140625}8}}"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m environment \u001b[38;5;241m=\u001b[39m escape\u001b[38;5;241m.\u001b[39menv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_cycles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monline_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mplay\u001b[1;34m(env, model, loss_fn)\u001b[0m\n\u001b[0;32m     15\u001b[0m rewards_list, grads_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m---> 18\u001b[0m     obs, rew, term, trunc \u001b[38;5;241m=\u001b[39m \u001b[43mone_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     rewards_list\u001b[38;5;241m.\u001b[39mappend(rew[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprisoner\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrew\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m, in \u001b[0;36mone_step\u001b[1;34m(env, obs, model)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step\u001b[39m(env, obs, model):\n\u001b[0;32m      3\u001b[0m     prisoner_inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprisoner\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m----> 4\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprisoner_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     predicted_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(tf\u001b[38;5;241m.\u001b[39margmax(output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      7\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprisoner\u001b[39m\u001b[38;5;124m\"\u001b[39m: predicted_action}\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:446\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    444\u001b[0m ):\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:629\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 629\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    631\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    632\u001b[0m         dataset\n\u001b[0;32m    633\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:638\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:243\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m options\u001b[38;5;241m.\u001b[39mexperimental_distribute\u001b[38;5;241m.\u001b[39mauto_shard_policy \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    240\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAutoShardPolicy\u001b[38;5;241m.\u001b[39mDATA\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    242\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mwith_options(options)\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1259\u001b[0m, in \u001b[0;36mDatasetV2.prefetch\u001b[1;34m(self, buffer_size, name)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprefetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer_size, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetV2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a `Dataset` that prefetches elements from this dataset.\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \n\u001b[0;32m   1234\u001b[0m \u001b[38;5;124;03m  Most dataset input pipelines should end with a call to `prefetch`. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;124;03m    A new `Dataset` with the transformation applied as described above.\u001b[39;00m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1259\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprefetch_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m   1260\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\prefetch_op.py:28\u001b[0m, in \u001b[0;36m_prefetch\u001b[1;34m(input_dataset, buffer_size, name)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[0;32m     27\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m input_dataset\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_PrefetchDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\prefetch_op.py:46\u001b[0m, in \u001b[0;36m_PrefetchDataset.__init__\u001b[1;34m(self, input_dataset, buffer_size, slack_period, name)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# We colocate the prefetch dataset with its input as this collocation only\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# happens automatically in graph mode.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m---> 46\u001b[0m   variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefetch_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43mslack_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslack_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:5990\u001b[0m, in \u001b[0;36mprefetch_dataset\u001b[1;34m(input_dataset, buffer_size, output_types, output_shapes, slack_period, legacy_autotune, buffer_size_min, metadata, name)\u001b[0m\n\u001b[0;32m   5988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   5989\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5990\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5991\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrefetchDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5992\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5993\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslack_period\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslack_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlegacy_autotune\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegacy_autotune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5994\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuffer_size_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   5996\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "environment = escape.env(render_mode=\"human\", max_cycles=1500)\n",
    "r = play(environment, online_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-4.28436905e-01, -3.37902725e-01,  1.75262526e-01,\n",
       "          1.59658149e-01, -3.47845346e-01, -4.24022824e-01,\n",
       "         -3.23054940e-01, -3.44390690e-01,  1.94203928e-01,\n",
       "          2.02639088e-01,  1.67316109e-01,  2.08915189e-01,\n",
       "          1.66561127e-01, -5.13572991e-01, -3.44702214e-01,\n",
       "         -4.68710601e-01, -3.26345533e-01, -3.29994023e-01,\n",
       "         -2.70585805e-01,  2.16544166e-01, -3.47773582e-01,\n",
       "          2.05544785e-01, -3.30572575e-01,  2.97297508e-01,\n",
       "         -3.36900383e-01,  1.99405923e-01,  1.60992369e-01,\n",
       "          1.83043361e-01,  2.17266724e-01,  2.04180270e-01,\n",
       "          3.46359104e-01,  1.55976534e-01],\n",
       "        [ 5.13253249e-02,  4.16800082e-02, -1.86015870e-02,\n",
       "         -1.68053824e-02,  4.67197523e-02,  3.85206155e-02,\n",
       "          4.19260897e-02,  3.50784846e-02, -1.30243925e-02,\n",
       "         -1.86298955e-02, -1.84092037e-02, -1.29842293e-02,\n",
       "         -2.25767773e-02,  2.72953045e-02,  3.78932580e-02,\n",
       "          4.11088020e-02,  4.33400869e-02,  3.96983065e-02,\n",
       "          4.00255211e-02, -2.26001348e-02,  3.12255286e-02,\n",
       "         -1.59662012e-02,  5.31505048e-02, -3.13292854e-02,\n",
       "          5.24505004e-02, -1.95536856e-02, -2.40805820e-02,\n",
       "         -2.32999269e-02, -2.13012900e-02, -3.06879450e-02,\n",
       "         -4.03942950e-02, -1.77794974e-02],\n",
       "        [-4.77810716e-03,  5.11875860e-02, -1.44775465e-01,\n",
       "         -1.52870223e-01,  4.21352163e-02,  6.14835974e-03,\n",
       "          3.98857519e-02,  4.76177782e-02, -1.37379691e-01,\n",
       "         -1.05057165e-01, -1.33095279e-01, -1.09270394e-01,\n",
       "         -1.39966384e-01, -4.10561711e-02,  3.84223498e-02,\n",
       "         -2.71214563e-02,  5.82151972e-02,  5.34768626e-02,\n",
       "          7.49737024e-02, -1.09779619e-01,  3.04887313e-02,\n",
       "         -1.22628644e-01,  3.93871479e-02, -7.41758645e-02,\n",
       "          3.94770131e-02, -1.15957402e-01, -1.48256972e-01,\n",
       "         -1.31812155e-01, -1.08347021e-01, -1.16771325e-01,\n",
       "         -4.30137105e-02, -1.56991422e-01],\n",
       "        [-1.12082437e-02, -6.82649296e-03, -7.96740130e-03,\n",
       "         -1.27189681e-02, -2.19576545e-02, -2.72655743e-03,\n",
       "         -1.79894306e-02, -2.09455192e-02, -1.89138367e-03,\n",
       "         -1.86825648e-03, -6.52304664e-03, -1.49426721e-02,\n",
       "         -1.81623939e-02,  2.82323826e-03, -1.59833133e-02,\n",
       "          7.17419200e-03, -1.78021658e-02, -1.69351026e-02,\n",
       "         -1.05699925e-02, -2.09757942e-04, -6.43000333e-03,\n",
       "         -1.27098076e-02, -1.51762562e-02,  7.22658494e-03,\n",
       "         -1.59436464e-02, -3.96291772e-03, -3.42977396e-03,\n",
       "         -1.54409716e-02, -4.74733673e-03, -2.99012661e-03,\n",
       "          8.45141150e-03, -1.39520448e-02]], dtype=float32),\n",
       " array([-0.00650733, -0.01260427,  0.02745356,  0.02931752, -0.0069647 ,\n",
       "        -0.01285146, -0.00374135, -0.0090293 ,  0.02899824,  0.0195757 ,\n",
       "         0.02412735,  0.03023572,  0.03107541, -0.0065726 , -0.00734646,\n",
       "        -0.01267119, -0.0101222 , -0.0097923 , -0.01070034,  0.02292074,\n",
       "        -0.01455072,  0.0329741 , -0.00562884,  0.02189314, -0.00563303,\n",
       "         0.02933693,  0.02902749,  0.03168208,  0.02442713,  0.02256405,\n",
       "         0.01827017,  0.03059111], dtype=float32),\n",
       " array([[-0.00860228, -0.02736274, -0.00614212, ...,  0.08944789,\n",
       "          0.00959851,  0.00474871],\n",
       "        [-0.05184268, -0.02875219,  0.03941615, ...,  0.04838192,\n",
       "          0.0390031 ,  0.05037905],\n",
       "        [ 0.13965021,  0.03780891, -0.10172062, ..., -0.01399604,\n",
       "         -0.0893592 , -0.10452974],\n",
       "        ...,\n",
       "        [ 0.11502226,  0.02703346, -0.07435243, ..., -0.02686934,\n",
       "         -0.08078624, -0.08492523],\n",
       "        [ 0.05238891,  0.03198053, -0.02513145, ..., -0.0623049 ,\n",
       "         -0.03246955, -0.03065517],\n",
       "        [ 0.14621034,  0.02873915, -0.09956645, ..., -0.00169923,\n",
       "         -0.10115425, -0.08997689]], dtype=float32),\n",
       " array([-0.07592291, -0.00321106,  0.024036  ,  0.02552703, -0.01955122,\n",
       "         0.00719719, -0.01510617,  0.02512769, -0.0284747 ,  0.00189183,\n",
       "        -0.00323749, -0.03896693,  0.03663531, -0.01226423,  0.05897141,\n",
       "         0.0469924 , -0.01226512, -0.01576363,  0.02069145,  0.028451  ,\n",
       "        -0.00800854, -0.01688663,  0.01951222, -0.00162568, -0.01949036,\n",
       "        -0.02024047, -0.00973263,  0.01404952,  0.00723525,  0.03996567,\n",
       "         0.04216544,  0.04457002], dtype=float32),\n",
       " array([[ 0.17964303,  0.0673921 , -0.46247587, -0.11553194,  0.42952996],\n",
       "        [ 0.20516731,  0.21165216,  0.01051136, -0.31024927,  0.1095236 ],\n",
       "        [-0.16213682, -0.3914741 ,  0.18393199, -0.33645317,  0.00144131],\n",
       "        [-0.23557006,  0.18113132, -0.29334792, -0.28440958, -0.38586992],\n",
       "        [-0.11641272, -0.3798497 , -0.10923673, -0.07944165,  0.26800093],\n",
       "        [-0.3745443 , -0.29934347, -0.07972179,  0.17341676,  0.03628702],\n",
       "        [ 0.09087697,  0.26344392, -0.3987829 ,  0.04060315, -0.13692585],\n",
       "        [ 0.09569713, -0.21266544, -0.04501017, -0.25599644, -0.47318602],\n",
       "        [ 0.22283167,  0.24952154, -0.05100362, -0.08611655,  0.37908497],\n",
       "        [ 0.30114588, -0.15999879,  0.25414068, -0.07574343, -0.07047134],\n",
       "        [-0.10550953, -0.27514908, -0.21302886, -0.46832418,  0.10160388],\n",
       "        [ 0.07164538, -0.34328118,  0.0126086 ,  0.21623607,  0.16037564],\n",
       "        [-0.18649611, -0.15108939, -0.0981079 , -0.17428549, -0.50648475],\n",
       "        [ 0.32728305,  0.05135104,  0.23642138,  0.09529893,  0.11393367],\n",
       "        [ 0.03885485,  0.0851457 ,  0.4400969 , -0.27392554,  0.13639435],\n",
       "        [ 0.08722454, -0.14943144,  0.18579215, -0.20894961, -0.31934202],\n",
       "        [ 0.06955175,  0.2553011 , -0.05826337,  0.2152041 ,  0.06477746],\n",
       "        [ 0.310817  , -0.11432252,  0.30254105,  0.29051334,  0.12655623],\n",
       "        [-0.04158695,  0.38049668,  0.24825425,  0.38420668, -0.08174951],\n",
       "        [ 0.05938314, -0.22763197,  0.4695857 , -0.14317124, -0.03869928],\n",
       "        [ 0.35614058,  0.24698803,  0.13157329,  0.34421793, -0.28259334],\n",
       "        [ 0.09401939, -0.32999337,  0.30705637,  0.1964916 ,  0.08216915],\n",
       "        [ 0.30396467, -0.01514758,  0.43309334, -0.11755345, -0.05683413],\n",
       "        [ 0.13986449,  0.38396707,  0.2504447 , -0.01387752,  0.34947973],\n",
       "        [-0.01955627,  0.28436536, -0.0745637 ,  0.3499436 ,  0.1566745 ],\n",
       "        [ 0.15420422,  0.37087265, -0.37799847, -0.05440789, -0.04657748],\n",
       "        [ 0.07087743, -0.2656015 , -0.0160654 , -0.03258413, -0.22911896],\n",
       "        [ 0.28492764,  0.00637282,  0.3729218 ,  0.24461316, -0.23660634],\n",
       "        [ 0.12931399, -0.40561622,  0.5825669 , -0.36839584,  0.20750637],\n",
       "        [ 0.03435208,  0.23851946, -0.03514228, -0.30074015, -0.2996126 ],\n",
       "        [-0.3349392 , -0.3068049 ,  0.41800025,  0.31340554, -0.37736243],\n",
       "        [-0.29636642, -0.22484659,  0.3439906 , -0.09060312, -0.14213304]],\n",
       "       dtype=float32),\n",
       " array([-0.03648108, -0.00568189,  0.04005324, -0.04918644, -0.08023915],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_model.get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
