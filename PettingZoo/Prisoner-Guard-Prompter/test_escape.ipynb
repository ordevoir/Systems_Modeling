{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "initializer = tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=[4]),\n",
    "    tf.keras.layers.Dense(20, activation=\"elu\", kernel_initializer=initializer,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(5, activation=\"sigmoid\", kernel_initializer=initializer,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совершаем 100 действий. Получаем 100 массивов длины 5 (`outputs`), содержащих прогнозы действий. Каждому действию соответствует отдача `dr`. Часть из них положительна, а часть отрицательна. На основе знаков формируем 100 массивов с эталонными значениями: берется спрогнозированный массив, и его максимум обнуляеся, если отдача отрицательна, либо, присваивается 1, если отдача положительна:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       "array([[0.1, 0.4, 0. , 0.3],\n",
       "       [0.1, 1. , 0.5, 0.3]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_targets(outputs, drs):\n",
    "    targets = np.array(outputs)\n",
    "    for i in range(len(drs)):\n",
    "        index = int(tf.argmax(outputs[i], axis=0))\n",
    "        targets[i, index] = 0. if drs[i] < 0.5 else 1.\n",
    "    return tf.Variable(targets)\n",
    "\n",
    "x = tf.Variable([[0.1, 0.4, 0.5, 0.3],\n",
    "                 [0.1, 0.7, 0.5, 0.3]])\n",
    "\n",
    "get_targets(x, [-2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_grads(model, input_, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(input_)\n",
    "        action = int(tf.argmax(output, axis=1))\n",
    "        target = np.array(output)\n",
    "        target[0, action] = 1.\n",
    "        target = tf.Variable(target)\n",
    "        loss = tf.reduce_mean(loss_fn(target, output))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    prisoner_inputs = tf.convert_to_tensor([obs[\"prisoner\"]])\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(prisoner_inputs)\n",
    "        probabilities = np.array([0.075, 0.075, 0.075, 0.075, 0.075])\n",
    "        predicted_action = int(tf.argmax(output, axis=1))\n",
    "        # add randomness for exploration:\n",
    "        probabilities[predicted_action] = 0.7\n",
    "        possible_actions = np.array([0, 1, 2, 3, 4])\n",
    "        prisoner_action = np.random.choice(possible_actions, p=probabilities)\n",
    "        # print(f\"{output = }\")\n",
    "        target = np.array(output)\n",
    "        target[0, prisoner_action] = 1.\n",
    "        target = tf.Variable(target)\n",
    "        loss = tf.reduce_mean(loss_fn(target, output))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    actions = {\"prisoner\": prisoner_action}\n",
    "    # print(f\"{actions = }\")\n",
    "    obs, rewards, term, trunc, infos = env.step(actions)\n",
    "    return obs, rewards, term, trunc, grads\n",
    "\n",
    "# @tf.function\n",
    "def play_one_episode(env, model, loss_fn):\n",
    "    obs, info = env.reset()\n",
    "    # obs, info = env.reset(seed=24)\n",
    "    rewards_list, grads_list = [], []\n",
    "\n",
    "    while env.agents:\n",
    "        obs, rew, term, trunc, grads = play_one_step(env, obs, model, loss_fn)\n",
    "        rewards_list.append(rew[\"prisoner\"])\n",
    "        grads_list.append(grads)\n",
    "    return rewards_list, grads_list\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    \"\"\"Функция возвращает отдачу (return)\"\"\"\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards)-2, -1 , -1):\n",
    "        discounted[step] += discounted[step+1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def get_final_grads(grads, advantages):\n",
    "    mean_grads = []\n",
    "    for layer in range(len(grads[0])):\n",
    "        layer_grads = []\n",
    "        for iter in range(len(grads)):\n",
    "            layer_grads.append(grads[iter][layer] * advantages[iter])\n",
    "        mean_grads.append(tf.reduce_mean(layer_grads, axis=0))\n",
    "    return mean_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(rewards, discount_factor):\n",
    "    \"\"\"Функция возвращает преимущества действий (action advantages)\"\"\"\n",
    "    discounted_rewards = discount_rewards(rewards, discount_factor)\n",
    "    reward_mean = discounted_rewards.mean()\n",
    "    reward_std = discounted_rewards.std()\n",
    "    \n",
    "    return (discounted_rewards - reward_mean) / reward_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from escape import env\n",
    "\n",
    "n_episodes = 1000000\n",
    "max_cycles = 102\n",
    "discount_factor = 0.92\n",
    "lr = 0.03\n",
    "loss_fn = tf.keras.losses.mse\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env(render_mode=None, max_cycles=max_cycles)\n",
    "for i in range(n_episodes):\n",
    "    # print(f\"\\r{(100 * i / n_episodes)}%\", end='')\n",
    "    print(f\"\\r{(i)}\", end='')\n",
    "    rewards, grads = play_one_episode(environment, model, loss_fn)\n",
    "    # dr = discount_rewards(rewards, discount_factor)\n",
    "    advantages = discount_and_normalize_rewards(rewards, 0.92)\n",
    "    mean_grads = get_final_grads(grads, advantages)\n",
    "    # print(f\"{mean_grads = }\")\n",
    "    optimizer.apply_gradients(zip(mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env(render_mode=\"human\", max_cycles=600)\n",
    "r, g = play_one_episode(environment, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    prisoner_inputs = tf.convert_to_tensor([obs[\"prisoner\"]])\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(prisoner_inputs)\n",
    "        probabilities = np.array([0.075, 0.075, 0.075, 0.075, 0.075])\n",
    "        predicted_action = int(tf.argmax(output, axis=1))\n",
    "        # add randomness for exploration:\n",
    "        probabilities[predicted_action] = 0.7\n",
    "        possible_actions = np.array([0, 1, 2, 3, 4])\n",
    "        prisoner_action = np.random.choice(possible_actions, p=probabilities)\n",
    "        # print(f\"{output = }\")\n",
    "        target = np.array(output)\n",
    "        target[0, prisoner_action] = 1.\n",
    "        target = tf.Variable(target)\n",
    "        loss = tf.reduce_mean(loss_fn(target, output))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    actions = {\"prisoner\": prisoner_action}\n",
    "    # print(f\"{actions = }\")\n",
    "    obs, rewards, term, trunc, infos = env.step(actions)\n",
    "    return obs, rewards, term, trunc, grads\n",
    "\n",
    "def play_one_episode(env, model, loss_fn):\n",
    "    # print(\"episode!\")\n",
    "    obs, info = env.reset()\n",
    "    # obs, info = env.reset(seed=24)\n",
    "    rewards_list, grads_list = [], []\n",
    "\n",
    "    while env.agents:\n",
    "        obs, rew, term, trunc, grads = play_one_step(env, obs, model, loss_fn)\n",
    "        rewards_list.append(rew[\"prisoner\"])\n",
    "        grads_list.append(grads)\n",
    "    return rewards_list, grads_list\n",
    "\n",
    "def play_multiple_episodes(env, model, loss_fn, n_episodes):\n",
    "    all_rewards, all_grads = [], []\n",
    "    for episode in range(n_episodes):\n",
    "        rewards, grads = play_one_episode(env, model, loss_fn)\n",
    "        all_rewards.append(rewards)\n",
    "        all_grads.append(grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    \"\"\"Функция возвращает отдачу (return)\"\"\"\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards)-2, -1 , -1):\n",
    "        discounted[step] += discounted[step+1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    \"\"\"Функция возвращает преимущества действий (action advantages)\"\"\"\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "def get_final_grads(grads, advantages):\n",
    "    mean_grads = []\n",
    "    for layer in range(len(grads[0])):\n",
    "        layer_grads = []\n",
    "        for iter in range(len(grads)):\n",
    "            layer_grads.append(grads[iter][layer] * advantages[iter])\n",
    "        mean_grads.append(tf.reduce_mean(layer_grads, axis=0))\n",
    "    return mean_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from escape import env\n",
    "\n",
    "n_iterations = 15000\n",
    "n_episodes_per_update = 10\n",
    "max_cycles = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_grads(all_grads, all_final_rewards):\n",
    "    all_mean_grads = []\n",
    "    for var_idx in range(len(model.trainable_variables)):\n",
    "        modified_grads = []\n",
    "        for episode_idx, final_rewards in enumerate(all_final_rewards):\n",
    "            for step, final_rwd in enumerate(final_rewards):\n",
    "                modified_grad = final_rwd * all_grads[episode_idx][step][var_idx]\n",
    "                modified_grads.append(modified_grad)\n",
    "        mean_grads = tf.reduce_mean(modified_grads)\n",
    "    all_mean_grads.append(mean_grads)\n",
    "    return all_mean_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ordevoir\\Documents\\GitHub\\Systems_Modeling\\PettingZoo\\Prisoner-Guard-Prompter\\escape.py:172: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. render_mode=\"rgb_array\" or render_mode=\"rgb_array\"\u001b[0m\n",
      "  warn(\"You are calling render method without specifying any render mode. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1245/15000, mean rewards: -312.2"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# print(f\"\\r{iteration+1}\", end='')\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     all_rewards, all_grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_multiple_episodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes_per_update\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# extra code – displays some debug info during training\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28msum\u001b[39m, all_rewards))\n",
      "Cell \u001b[1;32mIn[2], line 39\u001b[0m, in \u001b[0;36mplay_multiple_episodes\u001b[1;34m(env, model, loss_fn, n_episodes)\u001b[0m\n\u001b[0;32m     37\u001b[0m all_rewards, all_grads \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes):\n\u001b[1;32m---> 39\u001b[0m     rewards, grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     all_rewards\u001b[38;5;241m.\u001b[39mappend(rewards)\n\u001b[0;32m     41\u001b[0m     all_grads\u001b[38;5;241m.\u001b[39mappend(grads)\n",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m, in \u001b[0;36mplay_one_episode\u001b[1;34m(env, model, loss_fn)\u001b[0m\n\u001b[0;32m     28\u001b[0m rewards_list, grads_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m---> 31\u001b[0m     obs, rew, term, trunc, grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     rewards_list\u001b[38;5;241m.\u001b[39mappend(rew[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprisoner\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     33\u001b[0m     grads_list\u001b[38;5;241m.\u001b[39mappend(grads)\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mplay_one_step\u001b[1;34m(env, obs, model, loss_fn)\u001b[0m\n\u001b[0;32m     12\u001b[0m     target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(output)\n\u001b[0;32m     13\u001b[0m     target[\u001b[38;5;241m0\u001b[39m, prisoner_action] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m---> 14\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(loss_fn(target, output))\n\u001b[0;32m     17\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:197\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_variable_call\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_call):\n\u001b[1;32m--> 197\u001b[0m     variable_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m variable_call\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:1229\u001b[0m, in \u001b[0;36mVariable._variable_call\u001b[1;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape, experimental_enable_variable_lifting, **kwargs)\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggregation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1228\u001b[0m   aggregation \u001b[38;5;241m=\u001b[39m VariableAggregation\u001b[38;5;241m.\u001b[39mNONE\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprevious_getter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimport_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:1222\u001b[0m, in \u001b[0;36mVariable._variable_call.<locals>.<lambda>\u001b[1;34m(**kws)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Variable:\n\u001b[0;32m   1221\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1222\u001b[0m previous_getter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkws: \u001b[43mdefault_variable_creator_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, getter \u001b[38;5;129;01min\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39m_variable_creator_stack:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m   previous_getter \u001b[38;5;241m=\u001b[39m _make_getter(getter, previous_getter)\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:50\u001b[0m, in \u001b[0;36mdefault_variable_creator_v2\u001b[1;34m(next_creator, **kwds)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_variable_creator_v2\u001b[39m(next_creator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     48\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_variable_creator_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnext_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:355\u001b[0m, in \u001b[0;36mdefault_variable_creator_v2\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m shape \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    352\u001b[0m experimental_enable_variable_lifting \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperimental_enable_variable_lifting\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResourceVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimport_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:200\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m variable_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variable_call\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mVariableMetaclass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1829\u001b[0m, in \u001b[0;36mResourceVariable.__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape, handle, experimental_enable_variable_lifting)\u001b[0m\n\u001b[0;32m   1824\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_from_handle(trainable\u001b[38;5;241m=\u001b[39mtrainable,\n\u001b[0;32m   1825\u001b[0m                          shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   1826\u001b[0m                          dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1827\u001b[0m                          handle\u001b[38;5;241m=\u001b[39mhandle)\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1829\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_from_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1830\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1831\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1832\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1835\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1836\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1837\u001b[0m \u001b[43m      \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1838\u001b[0m \u001b[43m      \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1840\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1841\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1842\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1843\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:2028\u001b[0m, in \u001b[0;36mResourceVariable._init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape, validate_shape, experimental_enable_variable_lifting)\u001b[0m\n\u001b[0;32m   2026\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2027\u001b[0m   shape \u001b[38;5;241m=\u001b[39m initial_value\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m-> 2028\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43meager_safe_variable_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshared_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_in_graph_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2034\u001b[0m handle\u001b[38;5;241m.\u001b[39m_parent_trackable \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   2035\u001b[0m handle\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m handle_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:241\u001b[0m, in \u001b[0;36meager_safe_variable_handle\u001b[1;34m(initial_value, shape, shared_name, name, graph_mode)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a variable handle with information to do shape inference.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03mThe dtype is read from `initial_value` and stored in the returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m  The handle, a `Tensor` of type `resource`.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m dtype \u001b[38;5;241m=\u001b[39m initial_value\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_variable_handle_from_shape_and_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mgraph_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:169\u001b[0m, in \u001b[0;36m_variable_handle_from_shape_and_dtype\u001b[1;34m(shape, dtype, shared_name, name, graph_mode, initial_value)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInternalError(\n\u001b[0;32m    163\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    164\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    165\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing an explicit shared_name is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed when executing eagerly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m   shared_name \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39manonymous_name()\n\u001b[1;32m--> 169\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_handle_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshared_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m   initial_value \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[1;32mc:\\Users\\ordevoir\\miniconda3\\envs\\zoo\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:1276\u001b[0m, in \u001b[0;36mvar_handle_op\u001b[1;34m(dtype, shape, container, shared_name, debug_name, allowed_devices, name)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   1275\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1276\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVarHandleOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshared_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshared_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebug_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallowed_devices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_devices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   1281\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "environment = env(render_mode=None, max_cycles=max_cycles)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # print(f\"\\r{iteration+1}\", end='')\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        environment, model, loss_fn, n_episodes_per_update)\n",
    "\n",
    "    # extra code – displays some debug info during training\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    print(f\"\\rIteration: {iteration + 1}/{n_iterations},\"\n",
    "          f\" mean rewards: {total_rewards / n_episodes_per_update:.1f}\", end=\"\")\n",
    "\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_factor)\n",
    "    # all_mean_grads = []\n",
    "    # for var_index in range(len(model.trainable_variables)):\n",
    "    #     mean_grads = tf.reduce_mean(\n",
    "    #         [final_reward * all_grads[episode_index][step][var_index]\n",
    "    #          for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "    #              for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "    #     all_mean_grads.append(mean_grads)\n",
    "    all_mean_grads = modify_grads(all_grads, all_final_rewards)\n",
    "\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env(render_mode=\"human\", max_cycles=600)\n",
    "r, g = play_one_episode(environment, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18812776,  0.03559174, -0.11694428, -0.2694803 , -0.57455234,\n",
       "       -0.87962438, -1.18469642, -1.48976845, -1.79484049, -2.25244855,\n",
       "        0.64573581,  0.4931998 ,  0.34066378,  0.18812776,  0.03559174,\n",
       "       -0.11694428, -0.42201632, -0.72708836, -1.0321604 , -1.33723243,\n",
       "        1.71348795,  1.56095193,  1.40841591,  1.25587989,  1.10334387,\n",
       "        0.95080785,  0.79827183,  0.64573581,  0.4931998 ,  0.34066378])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards(r, 0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.18837774, -0.45670196, -0.286776  , -0.44300035, -0.0597306 ],\n",
       "        [ 0.1053976 , -0.22743143, -0.57153386, -1.0661036 ,  0.33794   ],\n",
       "        [ 0.6171772 ,  0.47629446,  1.1461575 , -0.01090528,  0.79948384],\n",
       "        [ 0.8132222 , -0.50877386, -0.53381246, -0.72459054,  0.905814  ],\n",
       "        [-0.61633545, -0.33643255, -1.0380995 , -0.92383546, -0.27554572]],\n",
       "       dtype=float32),\n",
       " array([-0.21911143, -0.90269554, -0.11212421,  0.09738877, -0.4334309 ],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
