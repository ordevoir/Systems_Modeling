{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from prisoner_guard_prompter import env\n",
    "\n",
    "parallel_env = env(render_mode=\"human\", max_cycles=1000)\n",
    "observations, infos = parallel_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В такой архитектуре, где выход первой нейронной сети бинаризуется и подается на вход второй нейронной сети, обратное распространение ошибки становится нетривиальной задачей из-за непрерывно-дискретного перехода. В стандартной ситуации обратное распространение ошибки основывается на непрерывности функций, что позволяет вычислять производные и обновлять веса через градиенты. Бинаризация, являясь дискретной операцией, нарушает этот процесс.\n",
    "\n",
    "Для решения этой проблемы обычно применяют метод, называемый **Straight-Through Estimator (STE)**. Этот метод позволяет приблизить градиенты через бинаризацию, чтобы всё равно можно было провести обратное распространение ошибки.\n",
    "\n",
    "Вот как это работает:\n",
    "\n",
    "1. **Прямой проход (forward pass)**:\n",
    "    - Первая нейронная сеть принимает входные данные и вычисляет свои выходные значения с сигмоидальной активацией.\n",
    "    - Эти значения бинаризуются (например, с помощью порога 0.5).\n",
    "    - Бинаризованные значения подаются на вход второй нейронной сети, которая решает задачу регрессии.\n",
    "\n",
    "2. **Обратный проход (backward pass)**:\n",
    "    - Вторая нейронная сеть вычисляет градиенты своей ошибки и обновляет свои веса стандартным способом.\n",
    "    - Градиенты ошибки передаются обратно к выходу первой сети.\n",
    "    - При обратном проходе через бинаризацию используется метод STE: мы принимаем производную бинаризации как производную сигмоидальной функции активации, что является приближением. То есть, производная сигмоида используется вместо нулевой производной бинаризации.\n",
    "\n",
    "В коде это можно представить следующим образом:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FirstNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FirstNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 8)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class SecondNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SecondNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(8, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.first_network = FirstNetwork()\n",
    "        self.second_network = SecondNetwork()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.first_network(x)\n",
    "        x = (x >= 0.5).float()  # бинаризация с порогом 0.5\n",
    "        x = self.second_network(x)\n",
    "        return x\n",
    "\n",
    "# Создание модели и определение функции потерь и оптимизатора\n",
    "model = CombinedModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Обучающий цикл\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Прямой проход\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Обратный проход и обновление весов\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "При таком подходе обратное распространение ошибки будет работать корректно, несмотря на присутствие бинаризации, благодаря использованию метода STE, который позволяет приближенно вычислять градиенты через дискретные шаги."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример на TF\n",
    "\n",
    "Для реализации описанного подхода с использованием TensorFlow, мы создадим две нейронные сети: первая сеть будет иметь 8 выходных нейронов с сигмоидальной функцией активации, а вторая сеть будет иметь один выходной нейрон для задачи регрессии. Мы будем использовать метод Straight-Through Estimator (STE) для выполнения обратного распространения через бинаризацию.\n",
    "\n",
    "Вот пример кода на TensorFlow:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "class FirstNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(FirstNetwork, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(8, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "class SecondNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SecondNetwork, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "class CombinedModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.first_network = FirstNetwork()\n",
    "        self.second_network = SecondNetwork()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.first_network(inputs)\n",
    "        # Бинаризация с порогом 0.5\n",
    "        x = tf.where(x >= 0.5, 1.0, 0.0)\n",
    "        return self.second_network(x)\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Создание модели\n",
    "model = CombinedModel()\n",
    "\n",
    "# Обучающий цикл\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(targets, outputs)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Пример данных\n",
    "import numpy as np\n",
    "input_size = 10\n",
    "num_samples = 1000\n",
    "\n",
    "x_train = np.random.random((num_samples, input_size)).astype(np.float32)\n",
    "y_train = np.random.random((num_samples, 1)).astype(np.float32)\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in dataset:\n",
    "        loss = train_step(inputs, targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataset)}\")\n",
    "```\n",
    "\n",
    "В этом коде:\n",
    "\n",
    "1. Мы создали две отдельные сети, `FirstNetwork` и `SecondNetwork`.\n",
    "2. Мы объединили эти сети в модель `CombinedModel`, где выходы первой сети бинаризуются перед подачей на вход второй сети.\n",
    "3. Использовали функцию `tf.where` для бинаризации выходов первой сети.\n",
    "4. Реализовали обучающий цикл с помощью `tf.GradientTape` для вычисления и применения градиентов, что позволяет обратное распространение ошибки через бинаризацию, используя STE.\n",
    "\n",
    "Этот подход обеспечивает корректное обучение нейронной сети с бинаризацией промежуточных выходов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
