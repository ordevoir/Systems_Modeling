{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple\n",
    "\n",
    "В этой среде один агент видит ориентир (**landmark**), и его вознаграждение определяется тем, насколько близко он подбирается к ориентиру. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " env.agents = ['agent_0']\n",
      " action_space = Discrete(5)\n",
      " observation_space = Box(-inf, inf, (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_v3\n",
    "\n",
    "env = simple_v3.env(max_cycles=150, render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "action_space = env.action_space(\"agent_0\")\n",
    "observation_space = env.observation_space(\"agent_0\")\n",
    "\n",
    "print(f\"{ env.agents = }\")\n",
    "print(f\"{ action_space = }\")\n",
    "print(f\"{ observation_space = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation space\n",
    "\n",
    "Агент может наблюдать 4 значения: первая пара значений – его собственная скорость (`self_vel`), вторая пара значений – позиция ориентира относительно агента (`landmark_rel_position`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [-0.67672384 -1.12335   ]\n"
     ]
    }
   ],
   "source": [
    "observation = env.observe(\"agent_0\")\n",
    "self_vel, landmark_rel_position = observation[:2], observation[2:]\n",
    "print(self_vel, landmark_rel_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "Дейстиве определяется единственным числом, задающим направление ускорения:\n",
    "\n",
    "- 0 – без ускорения ;\n",
    "- 1 – ускорение влево;\n",
    "- 2 – ускорение вправо;\n",
    "- 3 – ускорение вниз;\n",
    "- 4 – ускорение вверх.\n",
    "\n",
    "> При движении без ускорения скорость постепенно падает, а при постоянном ускорении вдоль одной из осей максимальная скорость ограничивается значением `2.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.sample()   # случайное значение действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e6c2620a10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  0. ] [-0.67672384 -1.12335   ]\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "observation = env.observe(\"agent_0\")\n",
    "self_vel, landmark_rel_position = observation[:2], observation[2:]\n",
    "print(self_vel, landmark_rel_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "Вознаграждение на каждом шаге определяется отрицательным значением квадрата евклидова расстояния агента до ориентира:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': -1.7198704141806709}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7198704296403378"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.rewards)\n",
    "landmark_rel_position[0]**2 + landmark_rel_position[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEC API\n",
    "\n",
    "Запустим Agent Environment Cycle ([AEC](https://pettingzoo.farama.org/api/aec/)) с хаотическими действиями. При запуске AEC действия агентов совершаются по очерди: в одной итерации совершается действие одного агента, на следующей итерации – действие второго агента и тд. Таким образом, агенты перебираются циклически.\n",
    "\n",
    "Метод `last()` возвращает значения для текущего агента, соответствующие его последнему действию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_v3.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample() # this is where you would insert your policy\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel API\n",
    "\n",
    "Параллельный запуск позволяет произвести действия для всех агентов в одной итерации. Для этого во внутреннем цикле (dict comprehension) перебираются агенты, и для каждого агента выбирается действие из его action space. Сформированный таким образом словарь (agent: action) передается методу `step()`, который возвращает значения для всех агентов, после их действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_env = simple_v3.parallel_env(render_mode=\"human\")\n",
    "observations, infos = parallel_env.reset(seed=42)\n",
    "\n",
    "while parallel_env.agents:\n",
    "    actions = {agent: parallel_env.action_space(agent).sample() for agent in parallel_env.agents}\n",
    "    observations, rewards, terminations, truncations, infos = parallel_env.step(actions)\n",
    "parallel_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a, b = [0, 1, 2], [3, 4, 5]\n",
    "np.concatenate([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': array([-7.9064537e-04,  1.0859103e+00, -4.3103251e-01,  1.5134330e-01],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_plicy(observation):\n",
    "    vx, vy, landmark_x, landmark_y = observation\n",
    "    if landmark_x > 0:\n",
    "        if vx < 1.0:\n",
    "            return 2\n",
    "    else:\n",
    "        if vx > -1.0:\n",
    "            return 1\n",
    "    if landmark_y > 0:\n",
    "        if vy < 1.0:\n",
    "            return 4\n",
    "    else:\n",
    "        if vy > -1.0:\n",
    "            return 3\n",
    "    return 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_env = simple_v3.parallel_env(max_cycles=25, render_mode=\"human\")\n",
    "observations, infos = parallel_env.reset(seed=42)\n",
    "\n",
    "while parallel_env.agents:\n",
    "    actions = {agent: simple_plicy(observations[\"agent_0\"]) for agent in parallel_env.agents}\n",
    "    observations, rewards, terminations, truncations, infos = parallel_env.step(actions)\n",
    "parallel_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
