{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple\n",
    "\n",
    "В этой среде один агент видит ориентир (**landmark**), и его вознаграждение определяется тем, насколько близко он подбирается к ориентиру. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " env.agents = ['agent_0']\n",
      " action_space = Discrete(5)\n",
      " observation_space = Box(-inf, inf, (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_v3\n",
    "\n",
    "env = simple_v3.env(max_cycles=150, render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "action_space = env.action_space(\"agent_0\")\n",
    "observation_space = env.observation_space(\"agent_0\")\n",
    "\n",
    "print(f\"{ env.agents = }\")\n",
    "print(f\"{ action_space = }\")\n",
    "print(f\"{ observation_space = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation space\n",
    "\n",
    "Агент может наблюдать 4 значения: первая пара значений – его собственная скорость (`self_vel`), вторая пара значений – позиция ориентира относительно агента (`landmark_rel_position`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.66235304 1.3075571 ]\n"
     ]
    }
   ],
   "source": [
    "observation = env.observe(\"agent_0\")\n",
    "self_vel, landmark_rel_position = observation[:2], observation[2:]\n",
    "print(self_vel, landmark_rel_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "Дейстиве определяется единственным числом, задающим направление ускорения:\n",
    "\n",
    "- 0 – без ускорения ;\n",
    "- 1 – ускорение влево;\n",
    "- 2 – ускорение вправо;\n",
    "- 3 – ускорение вниз;\n",
    "- 4 – ускорение вверх.\n",
    "\n",
    "> При движении без ускорения скорость постепенно падает, а при постоянном ускорении вдоль одной из осей максимальная скорость ограничивается значением `2.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.sample()   # случайное значение действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20a0223cd10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  0. ] [0.66235304 1.3075571 ]\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "observation = env.observe(\"agent_0\")\n",
    "self_vel, landmark_rel_position = observation[:2], observation[2:]\n",
    "print(self_vel, landmark_rel_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "Вознаграждение на каждом шаге определяется отрицательным значением квадрата евклидова расстояния агента до ориентира:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': -2.1484170799002817}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.148417133489829"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.rewards)\n",
    "landmark_rel_position[0]**2 + landmark_rel_position[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEC API\n",
    "\n",
    "Запустим Agent Environment Cycle ([AEC](https://pettingzoo.farama.org/api/aec/)) с хаотическими действиями. При запуске AEC действия агентов совершаются по очерди: в одной итерации совершается действие одного агента, на следующей итерации – действие второго агента и тд. Таким образом, агенты перебираются циклически.\n",
    "\n",
    "Метод `last()` возвращает значения для текущего агента, соответствующие его последнему действию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_v3.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample() # this is where you would insert your policy\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel API\n",
    "\n",
    "Параллельный запуск позволяет произвести действия для всех агентов в одной итерации. Для этого во внутреннем цикле (dict comprehension) перебираются агенты, и для каждого агента выбирается действие из его action space. Сформированный таким образом словарь (agent: action) передается методу `step()`, который возвращает значения для всех агентов, после их действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_env = simple_v3.parallel_env(render_mode=\"human\")\n",
    "observations, infos = parallel_env.reset(seed=42)\n",
    "\n",
    "while parallel_env.agents:\n",
    "    actions = {agent: parallel_env.action_space(agent).sample() for agent in parallel_env.agents}\n",
    "    observations, rewards, terminations, truncations, infos = parallel_env.step(actions)\n",
    "parallel_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a, b = [0, 1, 2], [3, 4, 5]\n",
    "np.concatenate([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': array([-0.3225102 ,  0.5560585 ,  0.04027966, -0.6605974 ], dtype=float32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_plicy(observation):\n",
    "    vx, vy, landmark_x, landmark_y = observation\n",
    "    if landmark_x > 0:\n",
    "        if vx < 1.0:\n",
    "            return 2\n",
    "    else:\n",
    "        if vx > -1.0:\n",
    "            return 1\n",
    "    if landmark_y > 0:\n",
    "        if vy < 1.0:\n",
    "            return 4\n",
    "    else:\n",
    "        if vy > -1.0:\n",
    "            return 3\n",
    "    return 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.2959244566672397})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.28149608239615587})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.2682776774217592})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.27429682754487783})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.24716026934606705})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.2188993795700935})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.18265843128535075})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.14919996601383892})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.1345340836969859})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.10677982314343211})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.08381686684974726})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.05774538495623137})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.038287698399676495})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.040545827463840306})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.04524915297772478})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.041452483099189016})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.024092517805505836})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.005247608928252496})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.001911436420303827})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.014766952093208243})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.019193931478767374})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.011891046178074991})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.0054756800544345655})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.016906602888837115})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03493636149319569})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.037843757582084706})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.023672286050912657})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.006192619512180211})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.003309648346405107})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.014572879795793968})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.018360253284021893})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.010886902061894237})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.004515587088318847})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.016074707818296473})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03463647538027086})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.037798523239489464})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.023737488265947224})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.006295824034545979})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.0034165046301803358})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.014577767309066951})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.018322202663366177})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.010835359587289976})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.004464335859491186})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.016029444309559795})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03462047841667664})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03779647694073072})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.02374144184775048})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.006301794365382448})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.0034226112514031135})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.014578092699294531})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.018320088112592243})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.010832472919432479})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.004461458649562628})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.016026900383675935})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03461958039595891})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03779636329525188})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.02374166538153549})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.006302131078400948})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.003422955419478782})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.014578111182220901})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.018319969124276816})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.010832310411346888})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.004461296652068091})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.016026757142192125})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03461952983421314})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.0377963569005139})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.02374167797234267})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.006302150041488638})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.003422974801689508})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.014578112223563875})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.018319962423910333})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.01083230126010503})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.004461287529509573})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.01602674907581123})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03461952698693197})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03779635654041973})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.023741678681384274})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.006302151109371815})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.0034229758931727567})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.014578112282207166})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.018319962046590046})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.01083230074476695})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.0044612870157865305})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.016026748621565143})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03461952682659165})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03779635652014165})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.023741678721312953})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.006302151169508104})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.003422975954638009})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.014578112285509549})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.01831996202534177})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.010832300715746427})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.0044612869868569825})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.016026748595984967})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.034619526817562356})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.03779635651899973})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.023741678723561477})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.006302151172894585})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.003422975958099335})\n",
      "rewards = defaultdict(<class 'int'>, {'agent_0': -0.01457811228569551})\n"
     ]
    }
   ],
   "source": [
    "parallel_env = simple_v3.parallel_env(max_cycles=100, render_mode=\"human\")\n",
    "observations, infos = parallel_env.reset(seed=42)\n",
    "\n",
    "while parallel_env.agents:\n",
    "    actions = {agent: simple_plicy(observations[\"agent_0\"]) for agent in parallel_env.agents}\n",
    "    observations, rewards, terminations, truncations, infos = parallel_env.step(actions)\n",
    "    print(f\"{rewards = }\")\n",
    "parallel_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent_0']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a, b = np.array([1, 2])\n",
    "a, b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
